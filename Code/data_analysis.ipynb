{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = [\"LLaMA3.2-1B\", \"LLaMA3.2-3B\", \"Qwen2.5-1.5B\", \"Qwen2.5-3B\"]\n",
    "\n",
    "info_df = pd.read_csv(\"results.csv\")\n",
    "outputs_df = pd.read_csv(\"results-main.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9819aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([info_df, outputs_df], axis=1)\n",
    "results_df.to_csv('complete_results.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af58f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model_names:\n",
    "    results_df[f'{name}_context_weights'] = results_df[f'{name}_context_weights'].apply(ast.literal_eval)\n",
    "    results_df[f'{name}_question_weights'] = results_df[f'{name}_question_weights'].apply(ast.literal_eval)\n",
    "    results_df[f'{name}_context_ave'] = results_df[f'{name}_context_ave'].apply(ast.literal_eval)\n",
    "    results_df[f'{name}_question_ave'] = results_df[f'{name}_question_ave'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def z_scale(data):\n",
    "    data = np.array(data, dtype='float32').reshape(-1, 1)\n",
    "    return scaler.fit_transform(data).reshape(1, -1)\n",
    "\n",
    "def get_similarity(llm_attn, human_attn):\n",
    "    normalized_llm = z_scale(llm_attn)\n",
    "    normalized_human = z_scale(human_attn)\n",
    "    return cosine_similarity(normalized_llm, normalized_human)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in model_names:\n",
    "    results_df[f'{name}_context_similarity'] = results_df.apply(lambda row: get_similarity(row[f'{name}_context_ave'], row[f'{name}_context_weights']), axis=1)\n",
    "    results_df[f'{name}_question_similarity'] = results_df.apply(lambda row: get_similarity(row[f'{name}_question_ave'], row[f'{name}_question_weights']), axis=1)\n",
    "    results_df[f'{name}_context_length'] = results_df['context'].apply(lambda x: len(x))\n",
    "    results_df[f'{name}_question_length'] = results_df['question'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_corrs = [0, 0, 0, 0]\n",
    "question_corrs = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    name = model_names[i]\n",
    "    context_corrs[i] = results_df[f'{name}_context_similarity'].corr(results_df[f'{name}_context_length'])\n",
    "    question_corrs[i] = results_df[f'{name}_question_similarity'].corr(results_df[f'{name}_question_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a890278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity average for all models\n",
    "similarities = [0, 0, 0, 0]\n",
    "for i in range(len(model_names)):\n",
    "    name = model_names[i]\n",
    "    similarities[i] = (results_df[f'{name}_context_similarity'].mean(), results_df[f'{name}_question_similarity'].mean())\n",
    "    print(similarities[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5949d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "labels = []\n",
    "for name in model_names:\n",
    "    cols.append(f'{name}_context_similarity')\n",
    "    labels.append(name)\n",
    "\n",
    "plot = results_df.boxplot(column=cols)\n",
    "plot.set_xticklabels(labels)\n",
    "plt.title('Context Attention Cosine Similarity Scores between LLMs and Humans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eeeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "labels = []\n",
    "for name in model_names:\n",
    "    cols.append(f'{name}_question_similarity')\n",
    "    labels.append(name)\n",
    "\n",
    "plot = results_df.boxplot(column=cols)\n",
    "plot.set_xticklabels(labels)\n",
    "plt.title('Question Attention Cosine Similarity Scores between LLMs and Humans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all grouped types\n",
    "types = [0, 0, 0, 0]\n",
    "for i in range(4):\n",
    "    types[i] = results_df[results_df['type'] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average attention and similarity to humans on context and question for all types for each model\n",
    "type_data = [pd.DataFrame() for _ in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    for name in model_names:\n",
    "        type_data[i][f'{name}_context_attention_{i}'] = types[i][f'{name}_context_ave'].apply(lambda x: sum(x) / len(x))\n",
    "        type_data[i][f'{name}_question_attention_{i}'] = types[i][f'{name}_question_ave'].apply(lambda x: sum(x) / len(x))\n",
    "        type_data[i][f'{name}_context_similarity_{i}'] = types[i][f'{name}_context_similarity']\n",
    "        type_data[i][f'{name}_question_similarity_{i}'] = types[i][f'{name}_question_similarity']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b227a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_summary = pd.DataFrame(columns=['type', 'model', 'avg_context_attn', 'avg_question_attn', 'avg_context_similarity', 'avg_question_similarity'])\n",
    "\n",
    "for i in range(4):\n",
    "    for name in model_names:\n",
    "        type_summary.loc[len(type_summary)] = [i, name, type_data[i][f'{name}_context_attention_{i}'].mean(), \n",
    "                                               type_data[i][f'{name}_question_attention_{i}'].mean(), type_data[i][f'{name}_context_similarity_{i}'].mean(), \n",
    "                                               type_data[i][f'{name}_question_similarity_{i}'].mean()]\n",
    "\n",
    "type_summary.sort_values(by=['model', 'avg_context_attn'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2199902",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = [0, 0, 0, 0]\n",
    "for x in range(len(model_names)):\n",
    "    name = model_names[x]\n",
    "    model_data[x] = pd.concat([type_data[t][[f'{name}_context_attention_{t}', f'{name}_question_attention_{t}', f'{name}_context_similarity_{t}', f'{name}_question_similarity_{t}']].reset_index(drop=True) for t in range(len(type_data))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "labels = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 7))\n",
    "\n",
    "for x in range(len(model_names)):\n",
    "    name = model_names[x]\n",
    "    cols.append([])\n",
    "    labels.append([])\n",
    "    for i in range(4):\n",
    "        cols[x].append(f'{name}_context_attention_{i}')\n",
    "        labels[x].append(f'Type {i}')\n",
    "    model_data[x].boxplot(column=cols[x], ax=axes[x//2, x%2])\n",
    "    axes[x//2, x%2].set_title(model_names[x])\n",
    "    axes[x//2, x%2].set_xticklabels(labels[x])\n",
    "    \n",
    "plt.suptitle('Context Attention for Different Context-Question Pair Types')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "labels = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 7))\n",
    "\n",
    "for x in range(len(model_names)):\n",
    "    name = model_names[x]\n",
    "    cols.append([])\n",
    "    labels.append([])\n",
    "    for i in range(4):\n",
    "        cols[x].append(f'{name}_question_attention_{i}')\n",
    "        labels[x].append(f'Type {i}')\n",
    "    model_data[x].boxplot(column=cols[x], ax=axes[x//2, x%2])\n",
    "    axes[x//2, x%2].set_title(model_names[x])\n",
    "    axes[x//2, x%2].set_xticklabels(labels[x])\n",
    "    \n",
    "plt.suptitle('Question Attention for Different Context-Question Pair Types')    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "labels = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 7))\n",
    "\n",
    "for x in range(len(model_names)):\n",
    "    name = model_names[x]\n",
    "    cols.append([])\n",
    "    labels.append([])\n",
    "    for i in range(4):\n",
    "        cols[x].append(f'{name}_context_similarity_{i}')\n",
    "        labels[x].append(f'Type {i}')\n",
    "    model_data[x].boxplot(column=cols[x], ax=axes[x//2, x%2])\n",
    "    axes[x//2, x%2].set_title(model_names[x])\n",
    "    axes[x//2, x%2].set_xticklabels(labels[x])\n",
    "\n",
    "plt.suptitle('Context Attention Cosine Similarity for Different Context-Question Pair Types')    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "labels = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 7))\n",
    "\n",
    "for x in range(len(model_names)):\n",
    "    name = model_names[x]\n",
    "    cols.append([])\n",
    "    labels.append([])\n",
    "    for i in range(4):\n",
    "        cols[x].append(f'{name}_question_similarity_{i}')\n",
    "        labels[x].append(f'Type {i}')\n",
    "    model_data[x].boxplot(column=cols[x], ax=axes[x//2, x%2])\n",
    "    axes[x//2, x%2].set_title(model_names[x])\n",
    "    axes[x//2, x%2].set_xticklabels(labels[x])\n",
    "\n",
    "plt.suptitle('Question Attention Cosine Similarity for Different Context-Question Pair Types')   \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5dd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering to remove model hallucinations\n",
    "import pandas as pd\n",
    "import ast\n",
    "from scipy.special import softmax\n",
    "\n",
    "annotated_results_df = pd.read_csv('annotated_complete_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Qwen2.5-1.5B\"\n",
    "filtered_df = annotated_results_df[annotated_results_df[f'{name}_hallucinated'] == False]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def z_scale(data):\n",
    "    data = np.array(data, dtype='float32').reshape(-1, 1)\n",
    "    return scaler.fit_transform(data).reshape(1, -1)\n",
    "\n",
    "def get_similarity(llm_attn, human_attn):\n",
    "    normalized_llm = z_scale(llm_attn)\n",
    "    normalized_human = z_scale(human_attn)\n",
    "    return cosine_similarity(normalized_llm, normalized_human)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[f'{name}_context_weights'] = filtered_df[f'{name}_context_weights'].apply(ast.literal_eval)\n",
    "filtered_df[f'{name}_question_weights'] = filtered_df[f'{name}_question_weights'].apply(ast.literal_eval)\n",
    "filtered_df[f'{name}_context_ave'] = filtered_df[f'{name}_context_ave'].apply(ast.literal_eval)\n",
    "filtered_df[f'{name}_question_ave'] = filtered_df[f'{name}_question_ave'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c300b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[f'{name}_context_similarity'] = filtered_df.apply(lambda row: get_similarity(row[f'{name}_context_ave'], row[f'{name}_context_weights']), axis=1)\n",
    "filtered_df[f'{name}_question_similarity'] = filtered_df.apply(lambda row: get_similarity(row[f'{name}_question_ave'], row[f'{name}_question_weights']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cfe4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity average for non-hallucinated responses\n",
    "model = (filtered_df[f'{name}_context_similarity'].mean(), filtered_df[f'{name}_question_similarity'].mean())\n",
    "print(f'{name}: {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for name in model_names:\n",
    "    labels.append(name)\n",
    "\n",
    "plt.boxplot([results_df[f'{model_names[0]}_context_similarity'], results_df[f'{model_names[1]}_context_similarity'],\n",
    "            filtered_df[f'{model_names[2]}_context_similarity'], results_df[f'{model_names[3]}_context_similarity']], labels=labels)\n",
    "plt.title('Context Attention Cosine Similarity Scores between LLMs and Humans')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62cb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([results_df[f'{model_names[0]}_question_similarity'], results_df[f'{model_names[1]}_question_similarity'],\n",
    "            filtered_df[f'{model_names[2]}_question_similarity'], results_df[f'{model_names[3]}_question_similarity']], labels=labels)\n",
    "plt.title('Question Attention Cosine Similarity Scores between LLMs and Humans')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d1a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = model_names[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f557811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all grouped types\n",
    "types = [0, 0, 0, 0]\n",
    "for i in range(4):\n",
    "    types[i] = filtered_df[filtered_df['type'] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_data = [pd.DataFrame() for _ in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    type_data[i][f'{name}_context_attention_{i}'] = types[i][f'{name}_context_ave'].apply(lambda x: sum(x) / len(x))\n",
    "    type_data[i][f'{name}_question_attention_{i}'] = types[i][f'{name}_question_ave'].apply(lambda x: sum(x) / len(x))\n",
    "    type_data[i][f'{name}_context_similarity_{i}'] = types[i][f'{name}_context_similarity']\n",
    "    type_data[i][f'{name}_question_similarity_{i}'] = types[i][f'{name}_question_similarity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_summary = pd.DataFrame(columns=['type', 'model', 'avg_context_attn', 'avg_question_attn', 'avg_context_similarity', 'avg_question_similarity'])\n",
    "\n",
    "for i in range(4):\n",
    "    type_summary.loc[len(type_summary)] = [i, name, type_data[i][f'{name}_context_attention_{i}'].mean(), \n",
    "                                            type_data[i][f'{name}_question_attention_{i}'].mean(), type_data[i][f'{name}_context_similarity_{i}'].mean(), \n",
    "                                            type_data[i][f'{name}_question_similarity_{i}'].mean()]\n",
    "\n",
    "type_summary.sort_values(by=['model', 'avg_context_attn'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86daac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = model_names[2]\n",
    "model_data = pd.concat([type_data[t][[f'{name}_context_attention_{t}', f'{name}_question_attention_{t}', f'{name}_context_similarity_{t}', f'{name}_question_similarity_{t}']].reset_index(drop=True) for t in range(len(type_data))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "cols = []\n",
    "labels = []\n",
    "\n",
    "for i in range(4):\n",
    "    cols.append(f'{name}_context_attention_{i}')\n",
    "    labels.append(f'Type {i}')\n",
    "model_data.boxplot(column=cols, ax=axes[0])\n",
    "axes[0].set_title('Context Attention')\n",
    "axes[0].set_xticklabels(labels)\n",
    "\n",
    "cols = []\n",
    "labels = []\n",
    "\n",
    "for i in range(4):\n",
    "    cols.append(f'{name}_question_attention_{i}')\n",
    "    labels.append(f'Type {i}')\n",
    "model_data.boxplot(column=cols, ax=axes[1])\n",
    "axes[1].set_title('Question Attention')\n",
    "axes[1].set_xticklabels(labels)\n",
    "\n",
    "plt.suptitle('Qwen2.5-1.5B Non-Hallucinated Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Qwen2.5-1.5B\"\n",
    "hallucinated_df = annotated_results_df[annotated_results_df[f'{name}_hallucinated'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucinated_df[f'{name}_context_weights'] = hallucinated_df[f'{name}_context_weights'].apply(ast.literal_eval)\n",
    "hallucinated_df[f'{name}_question_weights'] = hallucinated_df[f'{name}_question_weights'].apply(ast.literal_eval)\n",
    "hallucinated_df[f'{name}_context_ave'] = hallucinated_df[f'{name}_context_ave'].apply(ast.literal_eval)\n",
    "hallucinated_df[f'{name}_question_ave'] = hallucinated_df[f'{name}_question_ave'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucinated_df[f'{name}_context_similarity'] = hallucinated_df.apply(lambda row: get_similarity(row[f'{name}_context_ave'], row[f'{name}_context_weights']), axis=1)\n",
    "hallucinated_df[f'{name}_question_similarity'] = hallucinated_df.apply(lambda row: get_similarity(row[f'{name}_question_ave'], row[f'{name}_question_weights']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity average for hallucinated responses\n",
    "model = (hallucinated_df[f'{name}_context_similarity'].mean(), hallucinated_df[f'{name}_question_similarity'].mean())\n",
    "print(f'{name}: {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c010d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all grouped types\n",
    "hallu_types = [0, 0, 0, 0]\n",
    "for i in range(4):\n",
    "    hallu_types[i] = hallucinated_df[hallucinated_df['type'] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallu_type_data = [pd.DataFrame() for _ in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    hallu_type_data[i][f'{name}_context_attention'] = hallu_types[i][f'{name}_context_ave'].apply(lambda x: sum(x) / len(x))\n",
    "    hallu_type_data[i][f'{name}_question_attention'] = hallu_types[i][f'{name}_question_ave'].apply(lambda x: sum(x) / len(x))\n",
    "    hallu_type_data[i][f'{name}_context_similarity'] = hallu_types[i][f'{name}_context_similarity']\n",
    "    hallu_type_data[i][f'{name}_question_similarity'] = hallu_types[i][f'{name}_question_similarity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallu_type_summary = pd.DataFrame(columns=['type', 'model', 'avg_context_attn', 'avg_question_attn', 'avg_context_similarity', 'avg_question_similarity'])\n",
    "\n",
    "for i in range(4):\n",
    "    if hallu_types[i].shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    hallu_type_summary.loc[len(hallu_type_summary)] = [i, name, hallu_type_data[i][f'{name}_context_attention'].mean(), \n",
    "                                            hallu_type_data[i][f'{name}_question_attention'].mean(), hallu_types[i][f'{name}_context_similarity'].mean(), \n",
    "                                            hallu_types[i][f'{name}_question_similarity'].mean()]\n",
    "\n",
    "hallu_type_summary.sort_values(by=['model', 'avg_context_similarity'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ebc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 7))\n",
    "\n",
    "cols = []\n",
    "labels = ['Type 1 (Hallucinated)', 'Type 1', 'Type 3 (Hallucinated)', 'Type 3']\n",
    "\n",
    "for i in [1, 3]:\n",
    "    cols.append(hallu_type_data[i][f'{name}_context_attention'])\n",
    "    cols.append(model_data[f'{name}_context_attention_{i}'].dropna())\n",
    "axes[0].boxplot(cols, labels=labels)\n",
    "axes[0].set_title('Context Attention')\n",
    "\n",
    "cols = []\n",
    "for i in [1, 3]:\n",
    "    cols.append(hallu_type_data[i][f'{name}_question_attention'])\n",
    "    cols.append(model_data[f'{name}_question_attention_{i}'].dropna())\n",
    "axes[1].boxplot(cols, labels=labels)\n",
    "axes[1].set_title('Question Attention')\n",
    "\n",
    "plt.suptitle('Qwen2.5-1.5B Hallucinated vs Non-Hallucinated Average Attentions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 7))\n",
    "\n",
    "cols = []\n",
    "for i in [1, 3]:\n",
    "    cols.append(hallu_type_data[i][f'{name}_context_similarity'])\n",
    "    cols.append(model_data[f'{name}_context_similarity_{i}'].dropna())\n",
    "axes[0].boxplot(cols, labels=labels)\n",
    "axes[0].set_title('Context Similarity')\n",
    "\n",
    "cols = []\n",
    "for i in [1, 3]:\n",
    "    cols.append(hallu_type_data[i][f'{name}_question_similarity'])\n",
    "    cols.append(model_data[f'{name}_question_similarity_{i}'].dropna())\n",
    "axes[1].boxplot(cols, labels=labels)\n",
    "axes[1].set_title('Question Similarity')\n",
    "\n",
    "plt.suptitle('Qwen2.5-1.5B Hallucinated vs Non-Hallucinated Cosine Similarities')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
